{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "---\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LangSmith tracing only works when:\n",
    "---\n",
    "\n",
    "You're using LCEL (LangChain Expression Language) chains like:\n",
    "\n",
    "* prompt | model | parser\n",
    "\n",
    "* Or, you're calling invoke() on a full Runnable chain (not just ChatModel.invoke() directly).\n",
    "\n",
    "* Calling model.invoke(...) alone is too low-level — it doesn’t generate a trace in LangSmith unless it's wrapped inside a Runnable chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# invoking the model with a human message\n",
    "# The model will respond with a message based on the input provided\n",
    "# We can replace the content of the HumanMessage with any text you want to test\n",
    "result = model.invoke([\n",
    "    HumanMessage(content=\"Hi , My name is Irfan and I am currently learning GenAi\")\n",
    "])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# using this we can create a conversation\n",
    "# This is useful for testing how the model responds to different inputs\n",
    "# we do so by passing a list of messages to the model\n",
    "# thus our model remembers the context of the conversation\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Irfan and I am currently learning GenAi\"),\n",
    "        \n",
    "        AIMessage(content=\"That's great, Irfan! \\n\\nIt's wonderful to hear you're diving into the world of Generative AI. It's a fascinating and rapidly evolving field with a lot of potential. \\n\\nWhat specifically are you interested in learning about GenAI?  \\n\\nPerhaps you'd like to know more about:\\n\\n* **Different types of generative models:** Like text-to-image, text-to-code, or music generation?\\n* **How these models are trained:**  The role of large datasets and deep learning?\\n* **Applications of GenAI:**  In fields like art, writing, programming, or research?\\n* **Ethical considerations:**  Bias, fairness, and the impact of GenAI on society?\\n\\nTell me more about your goals, and I'll do my best to help you on your GenAI learning journey!\\n\"),\n",
    "        \n",
    "        HumanMessage(content=\"Hey, what's my name and what i am currently learning?\")\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "---\n",
    "- We can use a Message History class to wrap our model and make it stateful (so that it can remember the context w.r.t to any person talking with the model).\n",
    "-  This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. !\n",
    "### A session ID helps:\n",
    "- Track user interaction over time (within a conversation or chat window).\n",
    "\n",
    "- Maintain context between turns/messages in a conversation.\n",
    "\n",
    "- Optionally store state, preferences, history, or memory (if memory is enabled).\n",
    "\n",
    "### Note:\n",
    "-  By default, one session ID does NOT remember another session's conversation — unless you explicitly store and manage the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ChatMessageHistory  is a class that allows us to keep track of the messages exchanged in a chat session.\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# BaseChatMessageHistory is an abstract class that defines the interface for chat message histories.\n",
    "# It provides methods for adding messages, retrieving messages, and clearing the history.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "# RunnableWithMessageHistory is a class that wraps a model and allows it to maintain a message history.\n",
    "# It provides methods for invoking the model with a message history and retrieving the message history.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We will use a dictionary to store the session history for each session id\n",
    "# This will allow us to keep track of multiple chat sessions simultaneously\n",
    "# The session id will be used to distinguish one chat session from another\n",
    "store={}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This function retrieves the session history for a given session id.\n",
    "# If the session id does not exist in the store, it creates a new ChatMessageHistory instance and adds it to the store.\n",
    "# This way, we can ensure that each session has its own message history.\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        # If the session id does not exist, create a new ChatMessageHistory instance\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "\n",
    "# RunnableWithMessageHistory is used to wrap the model and provide it with the session history.\n",
    "# This allows the model to maintain a message history for each session and respond accordingly.\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is used to invoke the model with a session id\n",
    "# For now we are using a hardcoded session id\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config: This contains the session ID and maybe other metadata to track the user's conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the model with a human message and the session id\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Irfan and I am currently learning GenAi\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's ask the model a question that requires it to remember the context of the conversation.\n",
    "# This will test if the model can recall the information provided earlier in the conversation.\n",
    "# The model should be able to respond with the name and the topic of learning based on the previous messages.\n",
    "result = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config, # this is the session id we are using to distinguish one chat session from another\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing the config --> means changing the session id\n",
    "\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "# now my model will not remember the previous conversation\n",
    "# becase we are using a different session id\n",
    "#  and this session id does not have any previous messages stored\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, let's invoke the model with a different session id\n",
    "# This will create a new chat session and the model will not remember the previous conversation.\n",
    "# This is useful for testing how the model behaves in different chat sessions.\n",
    "# The model should respond with a message indicating that it does not remember the previous conversation.\n",
    "# This is because we are using a different session id, which does not have any previous messages\n",
    "# stored in the session history.\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model with a different session id\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "---\n",
    "- Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. \n",
    "\n",
    "- Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate,MessagesPlaceholder help us to create a prompt template that can be used to format the input messages for the model.\n",
    "# This allows us to define a system message and a placeholder for the human messages.\n",
    "# The system message provides context to the model, and the MessagesPlaceholder allows us to dynamically insert human messages into the prompt.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the best of your ability\"),\n",
    "\n",
    "        # MessagesPlaceholder uses a variable name to hold the human messages\n",
    "        # whatever human message we give its need to be in key value pairs, and the key name should be messages\n",
    "        \n",
    "        # MessagesPlaceholder is a placeholder for chat messages that lets you insert a list of messages (chat history) dynamically into a prompt template.\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  we now have a prompt template that can be used to format the input messages for the model.\n",
    "# This prompt template can be used to create a chain that combines the prompt with the model.\n",
    "# The chain will take the human messages as input and format them using the prompt template before passing\n",
    "# them to the model for generating a response.\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the chain with a human message\n",
    "# This will format the message using the prompt template and then pass it to the model for generating a response.\n",
    "# The model will respond with a message based on the input provided\n",
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is irfan\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now using the RunnableWithMessageHistory to wrap the chain\n",
    "# This allows us to maintain a message history for the chain and respond accordingly.\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is irfan\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Irfan\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_messages_key\n",
    "This tells LangChain:\n",
    "\n",
    "Which key in your input dict holds the list of messages ([HumanMessage, AIMessage, ...]) that should be passed to the MessagesPlaceholder in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Irfan\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing the Conversation History\n",
    "---\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "- 'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,         # ✅ Max total tokens allowed across the selected messages\n",
    "\n",
    "    strategy=\"last\",       # ✅ Trimming strategy: \n",
    "                           # \"last\" means keep the most recent messages and trim older ones first\n",
    "\n",
    "    token_counter=model,   # ✅ Token counter (usually the LLM object or tokenizer)\n",
    "                           # Used to count how many tokens each message will consume\n",
    "\n",
    "    include_system=True,   # ✅ Include the system message (if present) in trimming/calculation\n",
    "                           # System prompts are often important for guiding behavior\n",
    "\n",
    "    allow_partial=False,   # ✅ Don’t allow partial messages — either include a full message or not at all\n",
    "\n",
    "    start_on=\"human\"       # ✅ Start trimming/checking from the **last human message**\n",
    "                           # Useful when you want to prioritize keeping the most recent human turns\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter  # Used to extract specific keys from a dictionary\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "\n",
    "#  i am not getting because the trimmer trimmed the context of icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the Message History\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
